{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this lab is to present the tools for conducting Unsupervised learning and specifically clustering using Python.\n",
    "We will implement Kmeans and use sklearn to apply K-medoids, Kmeans, Hierarchical clustering and several clustering evaluation metrics. Linked with the topic of this lab is HW2. Additionally, this lab includes a practice assignment (under the name Lab2_practice.ipynb) for whomever is interested in starting practicing on unsupervised learning before HW2. The practice assignment is not mandatory and it will not be graded while the solutions will be discussed in Q&A session for lab2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "\n",
    "1. K-means (sklearn)\n",
    "2. K-means algorithm implementation (with random initialization) using pandas and numpy\n",
    "3. K-means caveats\n",
    "4. K-medoids\n",
    "5. Evaluation metrics\n",
    "\n",
    "- Clustering on iris dataset\n",
    "\n",
    "\n",
    "6. K-means, K-medoids, evaluation metrics and plotting with high-dimensional data\n",
    "7. Agglomerative Clustering and Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Data\n",
    "#make_blobs( )generates  Gaussian blobs ideal for clustering.\n",
    "#link: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
    "#the random state determines a random number generation for dataset creation. Essentially it generates a seed so that your results is always deterministic\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=4,\n",
    "                       cluster_std=0.60, random_state=0)\n",
    "\n",
    "#scatterplot to visualize the blobs\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, cmap='rainbow');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true are the integer labels for cluster membership of each sample\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kmeans (sklearn)\n",
    "\n",
    "K Means Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Unsupervised learning means that there is no outcome to be predicted, and the algorithm just tries to find patterns in the data. In k means clustering, we have the specify the number of clusters we want the data to be grouped into. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, it is relatively easy to pick out the four clusters. The k-means algorithm does this automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "#Initialize a KMeans object, parameters of KMeans include the number of clusters and the initialization method\n",
    "\n",
    "#Method for initialization: default is kmeans++\n",
    "\n",
    "#‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
    "\n",
    "#‘random’: choose n_clusters observations (rows) at random from data for the initial centroids.\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "#compute kmeans clustering\n",
    "kmeans.fit(X)\n",
    "#call the kmeans object, predict, to predict the cluster index of each sample. \n",
    "\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "#in one step: kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy array of the predicted index or labels for each cluster. Sample 0 is assigned to cluster 0, sample 1 is assigned to cluster 2, sample 2 is assigned to cluster 1 and so on.\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results by plotting the data colored by these labels. We will also plot the cluster centers as determined by the k-means estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "#the cluster centers are stored as an attribured of the kmeans object. It will give you the coordinates of the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "#so we can plot those too! and make them black so they stand out\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.9);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means implementation (with random initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)#generate a random number\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]#randomly permutate the df and get the 4 first elements, these are the first cluster centers-- essentially we suffle the data\n",
    "    centers = X[i]#store them to an array called centers\n",
    "\n",
    "\n",
    "    #Now that we generated the random centers we need to assign each point to one of these centers. To do that we need to calculate the distances between points and their closest        centers. we can use sklearns pairwise_distances_argmin method for that.\n",
    "\n",
    "    #link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Assign labels based on closest center\n",
    "        labels = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "\n",
    "\n",
    "        # 2b. Find new centers from means of points\n",
    "        # Next we need to find the new cendroid of each cluster and we can do that by calculating the mean of points\n",
    "\n",
    "        #so we can say. for i in range of the number of clusters, \n",
    "        #calculate the mean of these diffent clusters. so for the data points that belong in cluster 0 and the in cluster 1 and so on\n",
    "\n",
    "        # Update centroid location using the newly\n",
    "        # assigned data point classes\n",
    "        new_centers = np.array([X[labels == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers): #Test whether all array elements evaluate to True.\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, labels\n",
    "\n",
    "    \n",
    "#plot the centroids also\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.9);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CAVEATS of K-means\n",
    "1. if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results\n",
    "\n",
    "\n",
    "\n",
    "2. Another common challenge with k-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data. For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Because each iteration of k-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows. \n",
    "\n",
    "\n",
    "\n",
    "4. The fundamental model assumptions of k-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.In particular, the boundaries between k-means clusters will always be linear, which means that it will fail for more complicated boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to demonstrate bullet point 4:\n",
    "#This dataset from sklearn makes two interleaving half circles\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "Xx, yy = make_moons(200, noise=.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Run kmeans with n_clusters=2 and plot the results as we did before!\n",
    "\n",
    "#Kmeans completely failes to find the clusters as the boundaries here are not linear\n",
    "\n",
    "labels = KMeans(2, random_state=0).fit_predict(Xx)\n",
    "plt.scatter(Xx[:, 0], Xx[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of algorithm that would work on this dataset\n",
    "#DBSCAN \n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "clustering = DBSCAN(eps=0.3, min_samples=2).fit_predict(Xx)\n",
    "\n",
    "plt.scatter(Xx[:, 0], Xx[:, 1], c=clustering,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-medoids \n",
    "\n",
    "\n",
    "Kmedoids is similar to Kmeans, Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups). K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#K-medoids is not included in sklearn so download:\n",
    "!pip install scikit-learn-extra\n",
    "from sklearn_extra.cluster import KMedoids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmedoids also work with other metrics, like manhattan which is best though for binary data\n",
    "#here I demonstrate how you can change the metric in kmedoids\n",
    "kmedoids = KMedoids(n_clusters=4, random_state=0, metric=\"manhattan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroids plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmedoids.labels_,\n",
    "            s=50, cmap='viridis');\n",
    "\n",
    "centers = kmedoids.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.9);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation metrics: Intrinsic and Extrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the knowledge of Grounds truth labels with can use: adjusted rand index. If we do not have the class labels then we can use silhouette score. \n",
    "All the previously mentioned metrics can be found and implemented in python from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import pandas as pd \n",
    "\n",
    "#external measure, given that we have the class labels of the dataset\n",
    "#Computes how similar are the clusters to a set of given class labels\n",
    "#Measures the percentage of correct decisions taken by the clustering algorithm\n",
    "\n",
    "#and Silhouette score is an internal measure in the sense of that clustering is evaluated based on merely the data that\n",
    "#was used for the clustering, \n",
    "#The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "\n",
    "def evaluation_metrics(X, labels_pred, labels_true, metric_name):\n",
    "\n",
    "\n",
    "        s_s = metrics.silhouette_score(X, labels_pred, metric=metric_name)\n",
    "        a_r_i = metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "\n",
    "        metrics_names = [\"Silh_S\", \"Rand_Index\"]\n",
    "        values = [s_s, a_r_i]\n",
    "        \n",
    "\n",
    "        result = list(zip(metrics_names,values))\n",
    "        result = pd.DataFrame(result, columns=['Metric','Value'])\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Rand index of 1 measn that we have Perfectly matching labelings \n",
    "#for silhouette score where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "\n",
    "evaluation_metrics(X, kmeans.labels_, y_true, \"euclidean\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluation_metrics(X, kmedoids.labels_, y_true, \"manhattan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on iris dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-means, K-medoids, evaluation metrics and plotting with high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE that if your variables are of incomparable units, before clustering you need to standardize them! Example: some variables are in cm and some other are in kg. \n",
    "#if you would standardize the iris dataset for example:\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sd = StandardScaler()\n",
    "#X=sd.fit_transform(X)\n",
    "#and then you would use the standardized version of X to call the attribute fit_predict() of your clustering algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE that if you would like to find the optimal number of clusters for your dataset you could run the elbow method for your dataset. \n",
    "## Kmeans++\n",
    "### K-means\n",
    "kmeans_plus = KMeans(n_clusters = 3,init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "kmeans_plus.fit_predict(X)\n",
    "##check the init default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmedoids = KMedoids(n_clusters=3, random_state=0)\n",
    "kmedoids.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Kmeans \\n\", evaluation_metrics(X, kmeans_plus.labels_, y, \"euclidean\"))\n",
    "print(\"######\")\n",
    "print(\"######\")\n",
    "print(\"Kmedoids \\n\", evaluation_metrics(X, kmedoids.labels_, y, \"euclidean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting multi-dimensional datasets with more than two attributes is not as trivial. You can always reduce the dimensionality of the feature space by applying PCA on the dataset and plot the resulting principal components. \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit(X)\n",
    "pca_2d = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=y, s=50, cmap='viridis');\n",
    "plt.title(\"Reference plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans_plus.labels_, s=50, cmap='viridis');\n",
    "plt.title(\"K means\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "According to the evaluation metrics there should not be much of a difference between the results. And apparently there are a couple of points that Kmeans assigns them to one cluster while Kmedoids assigns them to the other cluster.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmedoids.labels_, s=50, cmap='viridis');\n",
    "plt.title(\"K medoids\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agglomerative Clustering and Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Hierarchical clustering\n",
    "# Ward is the default linkage algorithm, so we'll start with that\n",
    "ward = AgglomerativeClustering(n_clusters=3)\n",
    "ward_pred = ward.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's also try complete and average linkages\n",
    "\n",
    "The code below, conducts hierarchical clustering with complete linkage, stores the predicted labels in the variable complete_pred. It also conducts hierarchical clustering with average linkage and stores the predicted labels in the variable avg_pred\n",
    "\n",
    "Note: look at the documentation of AgglomerativeClustering for more information about the appropriate value to pass as the linkage value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering using complete linkage\n",
    "# Create an instance of AgglomerativeClustering with the appropriate parameters\n",
    "complete = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
    "# Fit & predict\n",
    "complete_pred = complete.fit_predict(X)\n",
    "\n",
    "\n",
    "\n",
    "# Hierarchical clustering using average linkage\n",
    "# Create an instance of AgglomerativeClustering with the appropriate parameters\n",
    "avg = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
    "# Make AgglomerativeClustering fit the dataset and predict the cluster labels\n",
    "avg_pred = avg.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_score = evaluation_metrics(X, ward_pred, y, \"euclidean\")\n",
    "complete_score = evaluation_metrics(X, complete_pred, y, \"euclidean\")\n",
    "avg_score = evaluation_metrics(X, avg_pred, y, \"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram visualization with scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The nice thing about hierarchical clustering is that it provides a complete dendrogram illustrating the relationships between clusters in our data. \n",
    "Let's visualize the highest scoring clustering result.\n",
    "\n",
    "To do that, we'll need to use Scipy's linkage function to perform the clustering again so we can obtain the linkage matrix it will later use to visualize the hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy's linkage function to conduct the clustering\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Specify the linkage type. Scipy accepts 'ward', 'complete', 'average', as well as other values\n",
    "# Pick the one that resulted in the highest Adjusted Rand Score\n",
    "linkage_type = 'average'\n",
    "\n",
    "linkage_matrix = linkage(X, linkage_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot using scipy's dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(22,18))\n",
    "\n",
    "# plot using 'dendrogram()'\n",
    "dendrogram(linkage_matrix)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#On the x axis you see labels which are the indices of your samples in X. On the y axis you see the distances (of the 'ward' method in our case).\n",
    "\n",
    "#Starting from each label at the bottom, you can see a vertical line up to a horizontal line. The height of that horizontal line tells you about the distance at which this label was #merged into another label or cluster. You can find that other cluster by following the other vertical line down again. If you don't encounter another horizontal line, it was just #merged with the other label you reach, otherwise it was merged into another cluster that was formed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a dendogram  to choose a number of the cluster for our data. Remember, a dendrogram only shows us the hierarchy of our data; it does not exactly give us the most optimal number of clusters.\n",
    "\n",
    "\n",
    "In order to identify clusters, we can cut the dendrogram horizontaly. The height of the cut to the dendrogram controls the number of clusters obtained. we can choose the cut-off point that cut the tallest vertical line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF LAB 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
