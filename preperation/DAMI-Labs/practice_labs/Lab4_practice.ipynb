{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! This lab has an objective to teach various ways of evaluating a model we already learned how to build in Lab 3. In the last lab, we did not perform any automatic process to determine the optimal parameter, but just put any value and saw whether the model showed a better perfor|mance. In this lab, you will learn how to perform a grid search for parameter settings. After that, in the following assignment, you will try various ways of evaluating a model. This lab contains the extra tasks to practice **after** completing the labs with videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4-3. Run more algorithms using scikit-learn\n",
    "  - Hold-out\n",
    "  - Repeated hold-out\n",
    "\n",
    "- 4-4. Implement manually\n",
    "  - Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3.  Run more algorithms using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. You will try two more methods related to splitting the datasets. After splitting, you will try to run a simple SVM method using the split datasets. Throughout this section, you will use the same training and test dataset we prepared above in the lab session! If you want to create it again, you can run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"sonar.all-data\", header=None)\n",
    "X = data.drop(60, axis=1)\n",
    "y = data[60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal holdout means splitting the dataset into two parts: A training set and a test set. Therefore, this method cannot be used when the task involves parameter tuning. In this case, we should use pre-defined parameters not based on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tried it once in the previous lab. Scikit-learn supports holdout method by **train_test_split** method in **model_selection** package. Let's load it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly run this method using our dataset (`X, y`). Let's set the test size to 0.2 (test_size), and apply stratifying. Do not forget to set *random_state* to our `RANDOM_SEED` defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an SVC instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model with a *training* dataset and its label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return the score with the fitted model and the *test* dataset and its label! Let's assign the value into the value called `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the method below to check if your answer is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_holdout(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A repeated holdout is somewhat similar to k-fold cross-validation. You can even say that it is a more generalized version of k-fold. It is running the normal holdout test multiple times by shuffling the dataset for each trial. In this way, you can come up with a more reasonable performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repeated holdout is supported by the class called **ShuffleSplit** located in the **model_selection** package. Let's load it first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ShuffleSplit method receives the following parameters: number of splits (`n_splits`), the proportion of a test set (`test_size`), and random state (`random_state`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this round, let's run the **cross_val_score** method we learned above. You can use this method with ShuffleSplit instance together. You may find the information somewhere in this lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a maximum score and mean cross-validation score and save them into corresponding variables here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = \n",
    "mean_score = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the method below to check if your answer is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_repeated_holdout(max_score, mean_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-4. Implement manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your last extra work in this lab is to try implementing grid search. For simplicity, we will not require you to develop cross-validation inside, since the core function of grid-search is to find out optimal parameter given the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pre-made the skeleton structure for you. You should develop *fit* and two member variables (*best_estimator_, best_score_*). You may want to use Python's `itertools.product` if you need to make your work easier. To evaluate each parameter, you should use **StratifiedKFold** inside to get the same result with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give you a parameter grid variable that was used above in our lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will again stick to a simple SVC classifier. But we do not need to initialize it now since we may need different instances for each iteration of parameter combinations. We used **class** notation and use **self** variable inside. This structure is made to give you the same experience with scikit-learn when testing. You only use **self** here when you want to call the methods defined in the class structure or want to access the class variable defined by self first. The class-based structure will not appear in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch_Manual():\n",
    "    def __init__(self, estimator, param_grid, cv):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.best_estimator_ = None\n",
    "        self.best_score_ = 0\n",
    "        self.cv = cv\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Step 1: You should compute all the possible permutations first. \n",
    "                Note that you cannot make a combination of values from different dictionary.\n",
    "        Step 2: For each permutation, you can now make a classifier instance SVC with the combined parameter set.\n",
    "        Step 3: Then you can run StratifiedKFold to train the model and test it using test set. \n",
    "                You should use the value self.cv as your k number. Calculate mean cross validation score\n",
    "        Step 4: You may need to compare your mean cross validation score every time to current best score.\n",
    "                If the best score changes, you need to save the new value into self.best_score_ and the classifier\n",
    "                into self.best_score_.\n",
    "        Step 5: Now after all iterations are finished, you should have the best estimator and score inside the instance.\n",
    "                The value should be identical to scikit-learn's value.\n",
    "        \"\"\"\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we made some callers for you. You should be able to initialize your instance in the same way as scikit-learn's **GridSearchCV** works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearch_Manual(estimator=SVC, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to fit and return the scores in the same way too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extra task is not graded, but let's check that your result is the same with the one from scikit_learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "gs.fit(X, y)\n",
    "print(grid.best_score_, grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of practice assignment 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
