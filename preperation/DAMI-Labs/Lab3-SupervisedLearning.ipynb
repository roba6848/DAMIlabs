{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! This lab has an objective to teach how to run simple classification models, mainly using the scikit-learn library. In this lab, you will learn how to build the models on a given training set and then apply them for predicting the values on a test set. Here in this lab we will also show you how to derive accuracy, one of the most famous performance measures, on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3-1. Run several algorithms using scikit-learn\n",
    "  - Perceptron\n",
    "  - K-nearest neighbors\n",
    "  - Decision tree\n",
    "  - Support vector machines\n",
    "\n",
    "- 3-2. Implement manually (for programmers)\n",
    "  - Perceptron\n",
    "  - K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Run several algorithms using scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may already know a basic concept of scikit-learn as we learn about it from the previous labs. We keep using it, but now our task is **supervised learning**, which means we start to deal with the labeled dataset!\n",
    "\n",
    "To use scikit-learn, we do not need to define python-style functions or classes; rather, we load and call the library methods directly.\n",
    "In this lab, we will use **Connectionist Bench** from UCI Machine Learning Repository, which can be downloaded [here](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data). We already located the dataset into **datasets** directory, so you can simply include it from there. This dataset has two classes: ***Mines***, ***Rocks*** with 60 attributes representing each data entity. More information can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)). If you succeed in downloading, place the file in the same directory with this jupyter lab file, and let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the libraries\n",
    "\n",
    "Basic libraries used throughout the lab session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "RANDOM_SEED = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you always need to do is loading data and check it correctly loaded. We will use pandas to load and manipulate it. Since there is no proper **head** for the table, you need to choose not to use the first row as a set of column names. The dataset is located in the **datasets** directory and its name is **sonar.all-data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"datasets/sonar.all-data\")\n",
    "data = pd.read_csv(\"datasets/sonar.all-data\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always check whether the shape of data by looking at first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the null values. You can use .info() function that you learned from the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also isnull() function to check nulls in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use scikit-learn, in which case we usually manage labels and data attributes separately. Let's separate the data labels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data.drop(60, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it might be better to split the training set and the test set now. In this lab, we will not apply any advanced validation strategy, such as k-fold cross-validation, so it will be enough just to split the whole dataset into two sets.\n",
    "\n",
    "To do this, we can use the **train_test_split** function in scikit_learn. You can find it in the model_selection package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method divides the entire data into a training set and a test set. To do this, we need to enter some parameters. Overall, we need to put what percentage our test set will have, whether we want to allow shuffling, random state, and whether we want to keep the label's proportions when we divide the data.\n",
    "\n",
    "In this lab, we will divide the training and test sets with a 70:30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm we are going to make is **Perceptron**! Perceptron is a binary classifier having one weight and one bias value $wâˆ™x+b$. You can also regard it as a single neuron classifier.\n",
    "\n",
    "Scikit-learn has this single neuron perceptron as its built-in function. We can try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron** is in the linear_model package of scikit_learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform analysis, we first need to make an instance by calling a class **Perceptron**. It receives few options, which can be found <a herf=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">here</a>. Since we are trying a basic perceptron we learned from the lecture, we do not need to put all the parameters they support. We will not use any regularization or early-stopping here, but there are still some parameters we need to care.\n",
    "\n",
    "- max_iter: Perceptron can converge or cannot converge; it depends on the dataset. So we can at least set some reasonable maximum iteration.\n",
    "- fit_intercept: Perceptron can have intercept (or bias) value or not. You can state it here (True/False).\n",
    "- tol: Since it is also possible that Perceptron is not converged forever, we can state a stopping criterion. The iteration will stop when loss > previous_loss - tol.\n",
    "- shuffle: We can shuffle the training data with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly create our instance with maximum iteration 100 and without shuffle and tol values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn = Perceptron(max_iter=100, tol=None, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already prepared training data, so we can call **fit** function with our dataset and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our instance has trained weight and bias. We can get *test error* with our test set by calling **score** function, and we can see predicted labels by calling **predict** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the **tol** parameter, the algorithm might finish earlier than our maximum iteration. We can also check it as it is stored in *n_iter_* variable in our instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm we will try is k-nearest neighbors or kNN. If we use scikit-learn again, all the methods we call will be the same. The only change we need to remember is when we create an instance because there will be different parameters. We can find kNN in the neighbors package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN is a simple algorithm having a small number of parameters. Detailed information can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). However, we will focus on the number of neighbors now as it is the critical factor of the performance. We can experiment by changing the number of neighbors. If needed, you can even change the distance function from euclidean to something else (p), and also, you can put more weight on the closest neighbor if the case is the numerical prediction (weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the instance, we can train and test our algorithm in the same way as Perceptron. We can use **fit** for training, **score** for getting the test accuracy, and **predict** for predicting the labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deal with the **decision tree** this time. When running a decision tree using scikit-learn, the process after creating an instance is the same as other classifiers. So, the most important thing to worry about is understanding and using the parameters used to create a new instance. You can find a normal decision tree in the **tree** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do not need to put any parameter as DecisionTreeClassifier has default options. Parameters are used to constraint the tree by limiting the maximum depth or minimum samples to split. There is no very optimal case that can be applied to all cases, so we may need to optimize it by running further optimization techniques such as *grid search*, which we will look into in the next lab. Here we are going to use the very normal decision tree without any parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making the instance, we can train and test our algorithm in the same way as Perceptron. We can use **fit** for training, **score** for getting the test accuracy, and **predict** for predicting the labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn offers a variety of support vector machine algorithms: SVC, NuSVC, and LinearSVC. SVC is the most basic form of support vector machine supporting various kernels, but LinearSVC forms a linear boundary without a kernel (You can find more [here](https://scikit-learn.org/stable/modules/svm.html)). NuSVC is similar to SVC, but the biggest feature is that we can adjust the number of support vectors. In this lab, we will use SVC, the most basic form of SVM. These three classifiers are available in the **SVM** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC** has a lot of parameters, similar to the decision tree we saw earlier. Many parameters are used to fine-tune the SVM. The most important parameter here is C, the regularization factor. This value is an indicator of how much the training set of the SVM can cover. The larger the C, the smaller the margin area, which means that the training set's fitting ability becomes stronger than before. Therefore, it is important to find C parameters that can find the right level of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the iris dataset by differing C! We prepared the plot_iris function that is available on the scikit-learn user guide website. We can try various C values and see the difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please import the function by running this block below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dami_dsv.supervised_learning.plot_iris import plot_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can freely change the C value and see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(C=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(C=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iris(C=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply SVM to our dataset. There are many available kernels, but in this lab, we will use the RBF kernel, which is set as a default in scikit-learn. We will have a chance to deal with other kernels in the upcoming assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma=\"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even change C and see the difference in the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C = 5, gamma=\"scale\")\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we give too much space for the margin, SVM can be underfitted, and it loses classification power. Too much generalization cannot always be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C = 0.1, gamma=\"scale\")\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Implement manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to implement some algorithms we tried before manually! It will give you a more robust understanding of the algorithm. We are going to implement **perceptron** and **kNN**!\n",
    "\n",
    "In those implementations, we used **class** notation and use **self** variable inside. This structure is made to give you the same experience with scikit-learn when testing. You only use **self** here when you want to call the methods defined in the class structure or want to access the class variable defined by self first. The class-based structure will not appear in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing perceptron, we need to change the letter classes into numbers as perceptron assumes that it receives binary numeric classes. If you want to make a more sophisticated one, then you can receive classes as they are and make some mapping function that maps the letters into binary numbers. However, here we will change them as they are not that important in this round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = y_train.replace('M', 0).replace('R', 1)\n",
    "y_test_new = y_test.replace('M', 0).replace('R', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we already have a basic structure of our new perceptron classifier! It has the same structure with scikit-learn's one, so we can try in the same way after finishing the development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self, max_iter):\n",
    "        \"\"\"\n",
    "        A constructor that receives parameters and save them into member variables.\n",
    "        You will receive max_iter value and need to save into self.max_iter.\n",
    "\n",
    "        Input:\n",
    "          max_iter: The maximum iteration of the algorithm.\n",
    "        Output:\n",
    "          None.\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        A method to train the model by receiving the training dataset and labels.\n",
    "\n",
    "        - Step 1: The algorithm needs to set an empty list of size |attributes|+1.\n",
    "                  The additional value is used for intercept value of the perceptron classifier.\n",
    "        - Step 2: The algorithm iterates self.max_iter times and train the model.\n",
    "        - Step 3: For each iteration, we traverse all rows in our dataset and predict the label of each row \n",
    "                  by calling self.predict method. If the label is different (prediction was wrong),\n",
    "                  we calculate the error by substracting a predicted label from a true label.\n",
    "        - Step 4: When prediction was wrong, we update the weights by adding [error*row] to the previous weights. \n",
    "                  For intercept value, we update it by simply adding the error to the previous value.\n",
    "        - Step 5: Save the trained weights into self.w.\n",
    "\n",
    "        Input:\n",
    "          X: Training dataset.\n",
    "          y: Training labels.\n",
    "\n",
    "        Output:\n",
    "          None.\n",
    "        \"\"\"\n",
    "        return\n",
    "                \n",
    "    def predict(self, d1):\n",
    "        \"\"\"\n",
    "        A method to predict a label with trained weights.\n",
    "\n",
    "        - Step 1: We calculate the dot product of our weights (self.w) and the given row.\n",
    "                  For the intercept value, we multiply this value by one since we do not have any value in the received row.\n",
    "        - Step 2: If the dot product is bigger than or equal to zero, return 1. Otherwise, return 0.\n",
    "\n",
    "        Input:\n",
    "          row: A single row from dataset.\n",
    "        Output:\n",
    "          Binary integer (0 or 1).\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        A method to calculate an accuracy score of a received dataset X and labels Y.\n",
    "\n",
    "        - Step 1: Set the initial loss value to zero.\n",
    "        - Step 2: We traverse all rows in our dataset and predict the label of each row\n",
    "                  by calling self.predict method. If the label is different (prediction was wrong),\n",
    "                  we add one to the loss value.\n",
    "        - Step 3: Divide the loss value by a size of the dataset and substract it from one to get an accuracy score.\n",
    "        - Step 4: Return the accuracy score.\n",
    "\n",
    "\n",
    "        Input:\n",
    "          X: Dataset that we want to calculate scores.\n",
    "          y: True labels for the dataset X.\n",
    "\n",
    "        Output:\n",
    "          score: An accuracy with a range of [0, 1].\n",
    "\n",
    "        \"\"\"\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self, max_iter):\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros(len(X.iloc[0])+1)\n",
    "        for it in range(self.max_iter):\n",
    "            loss = 0\n",
    "            for idx, row in X.iterrows():\n",
    "                prediction = self.predict(row)\n",
    "                error = y[idx] - prediction\n",
    "                loss += error\n",
    "                self.w[0] = self.w[0] + error\n",
    "                self.w[1:] = self.w[1:] + error * row\n",
    "                \n",
    "    def predict(self, row):\n",
    "        act = self.w[0]\n",
    "        act += self.w[1:].dot(row)\n",
    "        if act >= 0:\n",
    "            return 1\n",
    "        else: return 0\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        loss = 0\n",
    "        for idx, row in X.iterrows():\n",
    "            prediction = self.predict(row)\n",
    "            error = y[idx] - prediction\n",
    "            loss += abs(error)\n",
    "        return 1 - loss/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with implementation! Then we can create the instance, train the model, and test it in the same way!\n",
    "\n",
    "Run the codes below to check the result of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fit(X_train, y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the score is the same as the one we got from scikit-learn that we already tried in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.score(X_test, y_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is kNN's turn. To implement kNN easier, we may need **Counter**, one of the built-in data structures in Python collections. It is okay if you do not know it, but it makes your job much easier! If you want to know it, refer to Python's official document [here](https://docs.python.org/3/library/collections.html#counter-objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same structure again so that we can test in the same way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, n_neighbors):\n",
    "        \"\"\"\n",
    "        A constructor that receives parameters and save them into member variables.\n",
    "        You will receive n_neighbors value and need to save into self.n_neighbors.\n",
    "\n",
    "        Input:\n",
    "          n_neighbors: Number of neighbors to look when we predict.\n",
    "        Output:\n",
    "          None.\n",
    "        \"\"\"\n",
    "        return\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        A method to train the model by receiving the training dataset and labels.\n",
    "\n",
    "        - Step 1: Since we have no training process in KNN algorithm, we can just save the dataset and labels into\n",
    "                  the member variables self.X, self.y.\n",
    "                  \n",
    "        Input:\n",
    "          X: Training dataset.\n",
    "          y: Training labels.\n",
    "\n",
    "        Output:\n",
    "          None.\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def euclidean_dist(self, d1, d2):\n",
    "        \"\"\"\n",
    "        A method to calculate an euclidean distance between two data points d1 and d2.\n",
    "\n",
    "        - Step 1: We calculate an euclidean distance, by substracting one point from the other, \n",
    "                  square it and take a squared root.\n",
    "                  \n",
    "        Input:\n",
    "          d1, d2: Data points (rows) from the dataset.\n",
    "          \n",
    "        Output:\n",
    "          distance: An euclidean distance value between d1 and d2.\n",
    "        \"\"\"\n",
    "        return\n",
    "        \n",
    "    def predict(self, row):\n",
    "        \"\"\"\n",
    "        A method to predict a label with trained weights.\n",
    "\n",
    "        - Step 1: We iterate all training datasets and get [n_neighbors] nearest data points by calculating\n",
    "                  euclidean distances from the input data point to all training datasets.\n",
    "        - Step 2: We get the labels from self.y with the indices of [n_neighbors] nearest data points from self.X\n",
    "                  and perform majority vote on [n_neighbors] nearest data points' labels. \n",
    "                  In this stage, you can use collections.Counter to make this task easier.\n",
    "        - Step 3: Return the label that majority of the data points have.\n",
    "\n",
    "        Input:\n",
    "          row: A single row from dataset.\n",
    "        Output:\n",
    "          Binary integer (0 or 1).\n",
    "        \"\"\"\n",
    "        return\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        A method to calculate an accuracy score of a received dataset X and labels Y.\n",
    "\n",
    "        - Step 1: Set the initial loss value to zero.\n",
    "        - Step 2: We traverse all rows in our dataset and predict the label of each row \n",
    "                  by calling self.predict method. If the label is different (prediction was wrong),\n",
    "                  we add one to the loss value.\n",
    "        - Step 3: Divide the loss value by a size of the dataset and substract it from one to get an accuracy score.\n",
    "        - Step 4: Return the accuracy score.\n",
    "\n",
    "\n",
    "        Input:\n",
    "          X: Dataset that we want to calculate scores\n",
    "          y: True labels for the dataset X\n",
    "\n",
    "        Output:\n",
    "          score: An accuracy with a range of [0, 1]\n",
    "\n",
    "        \"\"\"\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def euclidean_dist(self, d1, d2):\n",
    "        #print(np.sqrt(np.sum((d1 - d2)**2)))\n",
    "        return np.sqrt(np.sum((d1 - d2)**2))\n",
    "        \n",
    "    def predict(self, d1):\n",
    "        distances = []\n",
    "        for idx, d2 in self.X.iterrows():\n",
    "            dist = self.euclidean_dist(d1, d2)\n",
    "            distances.append((idx, dist))\n",
    "        \n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        neighbors = []\n",
    "        for i in range(self.n_neighbors):\n",
    "            neighbors.append(self.y[distances[i][0]])\n",
    "        \n",
    "        #need to get labels\n",
    "        final_guess = Counter(neighbors).most_common(1)[0][0]\n",
    "        return final_guess\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        loss = 0\n",
    "\n",
    "        for idx, row in X.iterrows():\n",
    "            #print(row, len(row))\n",
    "            prediction = self.predict(row)\n",
    "            #rint(prediction, y[idx])\n",
    "            error = 1 if y[idx] == prediction else 0\n",
    "            loss += abs(error)\n",
    "        #rint(loss, len(y))\n",
    "        return loss/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test it and see if it returns the same score on our test dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the score is the same as the one we got from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF LAB 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
