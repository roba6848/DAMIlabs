{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Model Deployment\n",
    "\n",
    "At the end of this lab, you will learn how to export a trained machine learning model to reuse it in future classification tasks from python and from interactive web applications.\n",
    "\n",
    "This lab consists on the following tasks:\n",
    "\n",
    "1. Load and preprocess an example dataset\n",
    "1. Create a simple machine learning classifier\n",
    "1. Export a trained ML model to a computer file.\n",
    "1. Use the saved model to make predictions over unknown inputs\n",
    "1. Use the model directly from a web platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Find the dataset in the relative path with respect to the current file.\n",
    "\n",
    "As an example, we will classify three variaties of wheat seeds (Kama, Rosa, Canadian) from seven real-value attributes extracted from soft X-ray images:\n",
    "\n",
    "1. area A,\n",
    "1. perimeter P,\n",
    "1. compactness C = $\\frac{4*\\pi*A}{P^2}$,\n",
    "1. length of kernel LK,\n",
    "1. width of kernel WK,\n",
    "1. asymmetry coefficient A_Coef\n",
    "1. length of kernel groove LKG.\n",
    "\n",
    "More dataset info in https://archive.ics.uci.edu/ml/datasets/seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Relative path prefix to be able to find the dataset\n",
    "dataset_filename = \"./datasets/seed_data.csv\"\n",
    "print(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Since our objective is to show how to use the models in production, we are not going to apply any special preprocessing in the dataset, it is important that you apply a proper preprocessing pipeline to your own homework before exporting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize = (10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate in different variables the features and the Class label\n",
    "X = data.drop('target', axis=1)\n",
    "Y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier using scikit-learn\n",
    "\n",
    "In this case we are using a support vector classifier with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state=123, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "support_vector_classifier = SVC(kernel='linear')\n",
    "support_vector_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = support_vector_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the model\n",
    "\n",
    "So far, we have created a classifier that is stored the variable `support_vector_classifier`, which has an accuracy of $90\\%$.\n",
    "\n",
    "To export the trained model, we store a file in the hard drive that is going to use for production.\n",
    "\n",
    "For this purpose, we often use the package `pickle`, which is included by default in Python installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY: Relative path prefix to store the files in the \n",
    "FOLDER_PATH = \"dami_dsv/model_deployment/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_filename = FOLDER_PATH + \"trained_model_seeds_dataset.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE FILE WITH THE SPECIFIC VARIABLE IN THE SPECIFIED FOLDER\n",
    "data_to_save = support_vector_classifier\n",
    "file_path = trained_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a binary object and writes the indicated variables\n",
    "with open(file_path, \"wb\") as writeFile:\n",
    "    pickle.dump(data_to_save, writeFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model\n",
    "\n",
    "After the model has been stored, we can load the pickle file and assign it to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially trained model\n",
    "support_vector_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will load the same model, but in a variable that is completely empty\n",
    "loaded_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path is exactly the same than when we saved the file, it is written here just for clarification.\n",
    "trained_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "with open(trained_model_filename, \"rb\") as readFile:\n",
    "    loaded_model = pickle.load(readFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the model has the SVC classifier\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`THE MODEL WAS LOADED CORRECTLY FROM THE PICKLE FILE!`\n",
    "\n",
    "## Make predictions with the loaded trained model\n",
    "\n",
    "We can evaluate the loaded model to see that the performance metrics are the same than at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted_loaded_model = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test, Y_predicted_loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_predicted_loaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict a single sample\n",
    "#  [\"Area\", \"Perimeter\", \"Compactness\", \"Length of Kernel\", \n",
    "#           \"Width of Kernel\", \"Asymmetry Coeff.\", \"Length Kernel Groove\"]\n",
    "data_to_classify = [12.3, 13.34, 0.8684, 5.243, 2.974, 5.637, 5.063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colnames = data.columns\n",
    "colnames = colnames.drop('target').values\n",
    "print(colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame(data = [data_to_classify], columns = colnames)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = loaded_model.predict(sample)\n",
    "print(\"The predicted class for one sample is:\", prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy on a web application\n",
    "\n",
    "To follow the rest of the lab it is necessary to install the `Dash` package, documentation can be found on https://dash.plotly.com/: `pip install dash==1.13.3`. The specific version (1.13.3) of dash is needed to guarantee compatibility with provided codes.\n",
    "\n",
    "The next step is to run the following script in the terminal/console:  `python dami_dsv/model_deployment/dash_example_web.py`.\n",
    "\n",
    "After executing the file, dash will launch a web application in your computer with a message similar to:\n",
    "\n",
    "``` console\n",
    "Dash is running on http://127.0.0.1:8050/\n",
    "\n",
    " Warning: This is a development server. Do not use app.run_server\n",
    " in production, use a production WSGI server like gunicorn instead.\n",
    "\n",
    " * Serving Flask app \"dash_example_web\" (lazy loading)\n",
    " * Environment: production\n",
    "   WARNING: This is a development server. Do not use it in a production deployment.\n",
    "   Use a production WSGI server instead.\n",
    " * Debug mode: on\n",
    "```\n",
    "\n",
    "Leave the code running and open a tab in your internet browser. Access the webpage http://127.0.0.1:8050/ (or the one shown in the prompt before) and you will see an interactable web application that uses the classifier model. To close the application, click on the console/terminal and press `Ctrl+C` multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
