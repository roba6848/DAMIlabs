{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1-b: Data Exploration, Pre-processing, Plotting and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will guide your through the data exploration and data pre-processing process (inluding: plotting, data-cleaning, missing-values, outliers, standardization and dimensionality reduction) in Python. Linked with the topic of this lab is HW1. Additionally, this lab includes a practice assignment (under the name Lab1_practice.ipynb) for whomever is interested in starting practicing basic Python concepts before assignment 1. The practice assignment is not mandatory and it will not be graded while the solutions will be discussed in Q&A session for lab1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "\n",
    "1. Exploring the data\n",
    "\n",
    "2. Missing Values\n",
    "\n",
    "3. Imputation\n",
    "\n",
    "4. Plotting\n",
    "\n",
    "5. Outliers\n",
    "\n",
    "6. Highly correlated features\n",
    "\n",
    "7. Normalization vs Standardization\n",
    "\n",
    "8. Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "#download sklearn\n",
    "#We will use sklearn library during  this course. Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning \n",
    "!pip install sklearn\n",
    "#The ! tells the notebook to execute the cell as a shell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this session we will use the cleveland, heart disease dataset.  \n",
    "link to the dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"./datasets/processed.cleveland.data\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The name of the columns and generally information about this dataset can be found under the heart-disease.names file. \n",
    "\n",
    "set_axis assigns a  desired index to given axis. \n",
    "Input:\n",
    "    a list-like index\n",
    "    the axis to update\n",
    "    inplace: whether to return a new dataframe instance\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data.set_axis(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num' ], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information as found in heart-disease.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This database contains 14 attributes and 303 patients that might not or might have a heart disease diagnoses. \n",
    "\n",
    "There are both continuous and categorical attributes. \n",
    "\n",
    "For example, the dataset has the age of the patients, resting blood pressure, maximum heart rate for numerical features. \n",
    "While sex, chest pain type or number of major vessels are categorical features. \n",
    "\n",
    "The class label is categorical, consists of labels from 0 to 4 and refers to a diagnosis of a heart disease\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature explanation\n",
    "- age = in years // numerical, continuous\n",
    "\n",
    "- sex    =   (0 is female, 1 is male) // categorical\n",
    "\n",
    "\n",
    "- cp     = chest pain type (1 -> typical angina,  2 -> atypical angina,  3 -> non-anginal, 4 -> asymptomatic) //categorical\n",
    "\n",
    "- trestbps = resting blood pressure//continuous\n",
    "\n",
    "- chol      = serum cholestral in mg/dl //continuous\n",
    "\n",
    "- fbs       = fasting blood sugar > 120 mg/dl is 1 otherwise 0 //categorical\n",
    " \n",
    "- restecg   = resting electrocardiographic result, 0 -> normal, 1 -> St-T wave abnormality, 2 -> probable or definite hypertropy//categorical\n",
    "\n",
    "- thalach   = maximum heart rate achieved//continuous\n",
    "\n",
    "- exang     = exercise induced angina (1 = yes, 0 = no)//categorical\n",
    "\n",
    "- oldpeak   = ST depression induced by exercise relative to rest//continuous\n",
    "\n",
    "- slope     = the slope of the peak exercise ST segment (1 -> upslopping, 2 -> flat, 3 -> downslopping)//categorical\n",
    "\n",
    "- ca        = number of major vessels (0-3) covered by flourosopy//categorical\n",
    "\n",
    "- thal      = (3 -> normal, 6 -> fixed defect, 7 -> reversible defect)//categorical\n",
    "\n",
    "- class     = diagnosis of heart disease//categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Essentialy: \n",
    "#numerical are: age, trestbps, chol, thalac, oldpeak\n",
    "#categorical are: sex, cp, fbs, restecg, exang, slope, thal, class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of rows(patients) and colums(features)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check how many patients are diagnosed with Heart Failure. \n",
    "\n",
    "count_values(): returns a series containing counts of unique values\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data[\"num\"].value_counts()\n",
    "#count the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "There are 163 patients that do not have heart disease.\n",
    "While the patients that are postive to heart disease are distributed among labels 1 to 4.\n",
    "We can make the problem more balanced by transforming all these positive class labels to 1.\n",
    "\n",
    "\n",
    "Use the replace method to map the values  2, 3, 4 to 1!\n",
    "\n",
    "\"\"\"\n",
    "data[\"num\"].replace({2: 1, 3: 1, 4:1}, inplace=True)\n",
    "#alternative: data['num'] = np.where((data['num']>0),1,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's check the distribution again! the class now seems balanced.\n",
    "It's also binary. O for negative to heart disease and 1 for positive.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data[\"num\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When no data are stored. Missing data can have a significant effect on the conclusions and the ML process. We need to handle them!\n",
    "\n",
    "\n",
    "We can use the isna and isnull methods to check for Na, None and np.NaN\n",
    "\n",
    "\n",
    "# np.nan, None and NaT (for datetime64[ns] types) are standard missing value for Pandas.\n",
    "# np.nan and None are float so if you use them in a column of integers, they will be upcast to floating-point data type\n",
    "\n",
    "\n",
    "Write the name of the dataset and call isna() or is isnull(). These will return the original dataframe and will replace the values with True if there are any missing values. Otherwise it returns False. \n",
    "\n",
    "To summarize the results along a column call .any()\n",
    "\"\"\"\n",
    "\n",
    "print(data.isna().any()) #checks for NA, None, numpy.NaN\n",
    "print(data.isnull().any()) #checks for NaN, None, NaT in datetimelike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These methods say that there are no missing values. Remember that they only look for None and NaN values. \n",
    "\n",
    "But missing values come in many forms!\n",
    "\n",
    "Rule of thumb! Always open the dataset file and investigate the data yourself. \n",
    "Missing values might be written like: \"missing\", \"?\", \"_\", etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's print some information about the dataset before proceeding\n",
    "\n",
    "We can see that the attributes ca and thal are of object type.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print('Data Show Info\\n')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We notice that that the attributes, ca and thal are of object type. \n",
    "Let's see what is unique about them as compated to the others!\n",
    "Remember that ca and thal are catecorigal and are depicted as integers.\n",
    "\n",
    "We can print and see the unique values of ca and thal attribures. \n",
    "We notice that there are instances of ?.\n",
    "\n",
    "\n",
    "unique(): Returns unique values of Series object.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(data[\"ca\"].unique())\n",
    "print(data[\"thal\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle the missing values\n",
    "\n",
    "In general,  there are different ways of dealing with missing values\n",
    "\n",
    "For example:\n",
    "\n",
    " \n",
    "1. if the feature is nominal (categorical), replace missing with most frequent category\n",
    "2. if feature is numeric, replace it with Mean value\n",
    "3. or simply delete the rows that have missing values if there is an insignificant amount of them (The simplest approach for dealing with missing values is to remove entire predictor(s) and/or sample(s) that contain missing values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Count how many \"?\" the columns ca and thal have\n",
    "\n",
    "First index the columns that you are interested in and return their values by calling the values method. \n",
    "\n",
    "When you call  values in dataframe or series only the values will be returned, without the axes. \n",
    "\n",
    "See if any of these values in the columns are equal to ?\n",
    " \n",
    "and sum them!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(\"Number of missing (?) values for the whole dataframe:\\t\", sum(data.values==\"?\"))\n",
    "\n",
    "print(\"Number of missing (?) values for ca:\\t\", sum(data[\"ca\"].values==\"?\"))\n",
    "print(\"Number of missing (?) values for thal:\\t\", sum(data[\"thal\"].values==\"?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will use the Simple imputer from sklearn-impute\n",
    "\n",
    "1. First you need to initialize the SimpleImputer object, add a placeholder for missing values and an imputation strategy\n",
    "\n",
    "2. Create a new dataset and call the imputer and the fit_transform method. \n",
    "\n",
    "The fit method: computes the mean and std, to be used for later\n",
    "The transform method:  performs these calculations and transforms the dataset\n",
    "fit_transform: does the above two steps, in one step!\n",
    "\n",
    "Returns a numpy array!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#download sklearn first:\n",
    "#!pip install sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#first we define the object\n",
    "imputer = SimpleImputer(missing_values=\"?\", strategy='most_frequent')#it works along the columns\n",
    "#then fit it to the data\n",
    "data_imputed = imputer.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed\n",
    "#returns a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert it to a dataframe\n",
    "data_imputed = pd.DataFrame(data_imputed, columns = data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "#they were transformed to object type after the transformation\n",
    "#we'll transform all to float so we do not lose any information\\, the dataset is not so big so it's fine. \n",
    "\n",
    "\n",
    "astype(): Casts a pandas object to a specified dtype.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data_imputed=data_imputed.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In case you have empty values in the form of nan: \n",
    "dataframe.dropna(inplace=True)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#drop everything that contains ?\n",
    "\n",
    "data_dropped = data[(data[\"ca\"]!= \"?\") & (data[\"thal\"]!=\"?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dropped[['ca','thal']]=data_dropped[['ca','thal']].astype(float).astype(int)\n",
    "\n",
    "print('Data Show Info\\n')\n",
    "data_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To see statistical information about the dataset use the describe method. It will generate descriptive statistics \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print('Data Show Describe\\n')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Visualize the class distribution \n",
    "\n",
    "The figure object is your empty canvas.\n",
    "\n",
    "Next you can specify the limits of the plot frame by call .add_axes\n",
    "\n",
    "Then you can call ax.bar\n",
    "\n",
    "plt.style.use('ggplot'): the  \"ggplot\" style,  adjusts the style to emulate ggplot (a popular plotting package for R).\n",
    "\n",
    "Barplot parameters: \n",
    "    x : a sequence of scalars (unique values of the class label)\n",
    "    y: the height of the bars (how many samples each class has)\n",
    "\n",
    "add_axes(): Add an axes to the figure.\n",
    "    Input: The dimensions [left, bottom, width, height] of the new axes. All quantities are in fractions of figure width and height. \n",
    "    !left,bottom: the top left coordinates of your figure\n",
    "set_xticks(): Set the x ticks with list of ticks\n",
    "    Essentialy here we will pass the names of the bars. \n",
    "grid(): adds a grid to the plot\n",
    "        b: Whether to show the grid lines. True/False\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#first create a series where you will store the class label\n",
    "class_label = data_dropped[\"num\"]\n",
    "#replace the 0 with negative and 1 with positive, for the sake of interpretation\n",
    "class_label = class_label.replace({0: \"Negative\", 1: \"Positive\"})\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.bar(class_label.unique(), class_label.value_counts())\n",
    "ax.set_title(\"Distribution of class label\")\n",
    "ax.set_xticks(class_label.unique())\n",
    "\n",
    "\n",
    "ax.grid(b=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "2. plot the sex in relation to the class label\n",
    "\n",
    "Group the class label according to sex\n",
    "\n",
    "Groupby: allows you to group together rows based off if a column and perform an aggregate function on them\n",
    "After groupby, specify a summarization function!\n",
    "\n",
    "So groupby sex and count the values of male and female for each of the class labels. \n",
    "\n",
    "To plot: \n",
    "Call unstack, that pivots the grouped dataframe back, and just call plot with kind equals to bar!\n",
    "\n",
    "\n",
    "stacked: The bars for the different class labels will be put one top of each other, instead of next to each other. Convert it to False if you want to see the difference\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "sex_by_class = data_dropped.groupby(\"sex\").num.value_counts()\n",
    "sex_by_class.unstack().plot(kind='bar', stacked= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how groupby and unstack works\n",
    "sex_by_class\n",
    "#for each female and male we have the counts of the class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you call unstack the dataframe now is pivoted back!\n",
    "sex_by_class.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Same procedure: \n",
    "\n",
    "Distribution of age\n",
    "\"\"\"\n",
    "\n",
    "by_age= data_dropped.groupby([\"age\"]).num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_age.unstack().plot(kind='bar', stacked=True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pairplot from seaborn:  investigates pairwise relationships.\n",
    "\n",
    "Make a list of the numerical values, and in hue pass the class label!\n",
    "\n",
    "\n",
    "On the diagonal you see the distribution of these diffeent numerical variables\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(data_dropped[['age','trestbps','thalach','chol','num']],hue='num',size=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing steps\n",
    "\n",
    "1. Check and handle  Missing Values (We've done that already)\n",
    "2. Check for inconsistencies in the data (example: wrongly spelled words, abbreviations etc)\n",
    "3. Check for outliers\n",
    "4. Check for highly correlated features\n",
    "5. Standardize or Normalize numeric features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Check for outliers\n",
    "To check for outliers we can use the boxplot to see the distribution of the attributes. \n",
    "Any outliers are normally outside the plot region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Outliers can either be a mistake or just variance.\n",
    "If they are the result of a mistake, then we can ignore them, but if it is just a variance in the data we would need take into consideration the target population, subject-area, research question, and research methodology. \n",
    "\n",
    "\n",
    "!! **Rule of thumb**: Need to be careful before deleting the outliers, domain specific!\n",
    "\n",
    "\n",
    "If the outlier in question is:\n",
    "\n",
    "A measurement error or data entry error, correct the error if possible. If you can’t fix it, remove that observation because you know it’s incorrect.\n",
    "Not a part of the population you are studying (i.e., unusual properties or conditions), you can remove the outlier.\n",
    "A natural part of the population you are studying, you should not remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Create boxplots for all the numerical features\n",
    "\n",
    "Instead of plt.figure you can call plt.subplots and specify how many rows and columns you want.\n",
    "if nrows=1, ncols=2 it will create 1 row with 2 plots/ if nrows=2, ncols=3 it will create 2 rows with 3 plots each.\n",
    "\n",
    "Note!\n",
    "\n",
    "The axes attribute is just a list of the matplotlib axes. So you can actually iterate through and create  different plots!\n",
    "\n",
    "\n",
    "tight_layout(): automatically adjusts subplot params so that the subplot(s) fits in to the figure area\n",
    "\n",
    "\n",
    "barplot:\n",
    "    x: The input data.\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)#create 1 row with 4 plots\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax[0].set_title('trestbps')\n",
    "ax[0].boxplot(data[\"trestbps\"])\n",
    "\n",
    "ax[1].set_title('chol')\n",
    "ax[1].boxplot(data[\"chol\"])\n",
    "\n",
    "ax[2].set_title('thalach')\n",
    "ax[2].boxplot(data[\"thalach\"])\n",
    "\n",
    "ax[3].set_title('oldpeak')\n",
    "ax[3].boxplot(data[\"oldpeak\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. Highly correlated features\n",
    "\n",
    "\n",
    "Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation).\n",
    "\n",
    "\n",
    "A strong correlation is indicated by a Pearson Correlation Coefficient value near 1. Therefore, when looking at the Heatmap, we want to see what correlates most with the class label.\n",
    "\n",
    "The correlation coefficient has values between -1 to 1:\n",
    "\n",
    "— A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n",
    "\n",
    "\n",
    "— A value closer to 1 implies stronger positive correlation\n",
    "\n",
    "\n",
    "— A value closer to -1 implies stronger negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns \n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "cor = data_dropped.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the thalac has the strongest correlation with the class label!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Normalization vs Standardization \n",
    "\n",
    "The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1. \n",
    "https://www.statisticshowto.com/normalized/\n",
    "\n",
    "\n",
    "For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance. https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "\n",
    "\n",
    "Compare the effects of diffeent scalers from sklearn:\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Standardization transforms data to have a mean of zero and a standard deviation of 1. \n",
    "\n",
    "It is a crucial step before performing PCA, since we are interested in the components that maximize the variance. \n",
    "\n",
    "\n",
    "Make a list of the numerical features and extract them from the original dataset. \n",
    "\n",
    "Create our StandardScaler object\n",
    "\n",
    "\n",
    "fit: computes the mean and std to be used for later scaling. \n",
    "\n",
    "transform: uses a previously computed mean and std to autoscale the data \n",
    "\n",
    "fit_transform:  does both at the same time. So you can do it with 1 line of code instead of 2.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"] #list of num features\n",
    "\n",
    "\n",
    "X = data_dropped[[c for c in data_dropped.columns if c in numerical]] #list comprehension\n",
    "\n",
    "\n",
    "#Scale Data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#This: \n",
    "#X = data_dropped[[c for c in data_dropped.columns if c in numerical]]\n",
    "\n",
    "#can be considered equal to this: \n",
    "\n",
    "#S = pd.DataFrame()\n",
    "#for c in data_dropped.columns:\n",
    "    #if c in numerical:\n",
    "        #S[c] = data_dropped[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "PCA is an unsupervised statistical technique that aims to investigate the innerrelations among a set of variables and their underline structure. \n",
    "\n",
    "\n",
    "Overall PCA  attemps to find out what features explain the most variance in your data\n",
    "\n",
    "PCA also helps yo visualize your data. in our dataset we have 14 components which it's a bit impossible to plot. So we can reduce the dimensionality and plot the two principal components of the dataset\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Intatiate a pca object, specify how many principal components you want\n",
    "\n",
    "Fit your dataset\n",
    "Transform: and the apply the rotation and dimensionality reduction\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and then transform and store into a variable\n",
    "x_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's check the dimensions after pca \n",
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "\n",
    "#the first component explains 35% and the second 21%\n",
    "\n",
    "#Components are a linear transformation that chooses a variable system for the dataset such that the greatest variance of the dataset comes to lie on the first axis. \n",
    "#and likewise the second greatest variance lies on the second axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Scatterplot to plot the two principal components. \n",
    "\n",
    "Note:\n",
    "In general, interpreting the  components is not  easy. \n",
    "But based on the two components we can see if we have a clear seperation between the labels. \n",
    "\n",
    "For this dataset the seperation is not as clear. (in toy datasets you might be able to see clear groups forming)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=data_dropped['num'],cmap='rainbow')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "The components correspond to a combination of features from your dataset. \n",
    "The components themselves are stored as an attribute to the pca object\n",
    "\n",
    "\"\"\"\n",
    "pca.components_ \n",
    "# Each row represents a principal component and each column actually relates back to the original features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's transform this array into a df\n",
    "\n",
    "df_comp = pd.DataFrame(pca.components_,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we create a heatmap now we will be able to see the correlation between various features and the principal components themselves\n",
    "\n",
    "basically each principal comp is shown here as a row and the more yellow the color then it's more correlated to a feature in the column \n",
    "In that way you can see which features are more important for each principal component \n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df_comp,cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF LAB 1-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}